{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shams200648/Movie-Recommendation-System/blob/main/Movie_Recommendation_System.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 1: Download TMDB Dataset"
      ],
      "metadata": {
        "id": "cpeS2Fbromir"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJdv7oc9oWGN",
        "outputId": "5807b3cf-9ac9-4b16-b3d6-cd82dbe4845e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieving folder contents\n",
            "Processing file 1oOQrIqgHe1BLZDXggmlGrynwCu1L4BBl tmdb_5000_credits.csv\n",
            "Processing file 1uL6ziMoMH0rhOaJJeFD_dYYMm6URJ_7m tmdb_5000_movies.csv\n",
            "Retrieving folder contents completed\n",
            "Building directory structure\n",
            "Building directory structure completed\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1oOQrIqgHe1BLZDXggmlGrynwCu1L4BBl\n",
            "To: /content/Movie Dataset/tmdb_5000_credits.csv\n",
            "100% 40.0M/40.0M [00:00<00:00, 58.8MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1uL6ziMoMH0rhOaJJeFD_dYYMm6URJ_7m\n",
            "To: /content/Movie Dataset/tmdb_5000_movies.csv\n",
            "100% 5.70M/5.70M [00:00<00:00, 91.2MB/s]\n",
            "Download completed\n"
          ]
        }
      ],
      "source": [
        "!gdown --folder https://drive.google.com/drive/folders/1A2io9k2MXnjXEBrsOJTZGpi6mTs1ZjWb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q gdown\n",
        "print(\"âœ… gdown installed\")\n"
      ],
      "metadata": {
        "id": "jScNfYinGIR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 2: Install Required Dependencies"
      ],
      "metadata": {
        "id": "xWnCc4Sbopgn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install all required libraries\n",
        "!pip install -q sentence-transformers chromadb streamlit pyngrok pandas numpy scikit-learn\n",
        "print(\"âœ… All dependencies installed successfully!\")\n",
        "print(\"\\nInstalled packages:\")\n",
        "print(\"- sentence-transformers (for embeddings)\")\n",
        "print(\"- chromadb (vector database with HNSW)\")\n",
        "print(\"- streamlit (web UI)\")\n",
        "print(\"- pyngrok (public URL tunneling)\")"
      ],
      "metadata": {
        "id": "5s0XY1o9otxP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 3: Import Libraries"
      ],
      "metadata": {
        "id": "K8BUFlTJowor"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import ast\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import chromadb\n",
        "from chromadb.config import Settings\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pickle\n",
        "import time\n",
        "\n",
        "print(\"âœ… All libraries imported successfully!\")\n",
        "print(f\"Pandas version: {pd.__version__}\")\n",
        "print(f\"NumPy version: {np.__version__}\")"
      ],
      "metadata": {
        "id": "va618CNoo25y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 4: Load TMDB Dataset"
      ],
      "metadata": {
        "id": "jWCDdpUMo6CN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the two CSV files from Google Drive\n",
        "\n",
        "import gdown\n",
        "\n",
        "try:\n",
        "    movies_df = pd.read_csv('/content/Movie Dataset/tmdb_5000_movies.csv')\n",
        "    credits_df = pd.read_csv('/content/Movie Dataset/tmdb_5000_credits.csv')\n",
        "    print(\"âœ… Files loaded \")\n",
        "except:\n",
        "    # Fallback: download from Kaggle or provide instructions\n",
        "    print(\"âš ï¸ Files not found in Drive. Please upload:\")\n",
        "    print(\"1. tmdb_5000_movies.csv\")\n",
        "    print(\"2. tmdb_5000_credits.csv\")\n",
        "    print(\"\\nTo your Google Drive root or MyDrive folder\")\n",
        "\n",
        "print(f\"\\n Movies Dataset Shape: {movies_df.shape}\")\n",
        "print(f\" Credits Dataset Shape: {credits_df.shape}\")\n",
        "print(f\"\\nğŸ¬ Total Movies: {len(movies_df)}\")"
      ],
      "metadata": {
        "id": "XZrKExp9o-qP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 5: Explore Dataset Structure"
      ],
      "metadata": {
        "id": "mgiPSnM_pJYB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\"*80)\n",
        "print(\"MOVIES DATASET - First 3 rows\")\n",
        "print(\"=\"*80)\n",
        "print(movies_df.head(3))\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"CREDITS DATASET - First 3 rows\")\n",
        "print(\"=\"*80)\n",
        "print(credits_df.head(3))\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"MOVIES COLUMNS:\")\n",
        "print(\"=\"*80)\n",
        "print(movies_df.columns.tolist())\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"CREDITS COLUMNS:\")\n",
        "print(\"=\"*80)\n",
        "print(credits_df.columns.tolist())\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"DATA TYPES:\")\n",
        "print(\"=\"*80)\n",
        "print(movies_df.dtypes)"
      ],
      "metadata": {
        "id": "j8V9hBQkpK0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 6: Merge Datasets"
      ],
      "metadata": {
        "id": "q2BFKEvtpPw8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge movies and credits datasets\n",
        "# The key column is 'id' in movies_df and 'movie_id' in credits_df\n",
        "\n",
        "# First, let's check the common column names\n",
        "print(\"Checking merge keys...\")\n",
        "if 'id' in movies_df.columns and 'movie_id' in credits_df.columns:\n",
        "    df = movies_df.merge(credits_df, left_on='id', right_on='movie_id', how='inner')\n",
        "elif 'movie_id' in movies_df.columns and 'movie_id' in credits_df.columns:\n",
        "    df = movies_df.merge(credits_df, on='movie_id', how='inner')\n",
        "else:\n",
        "    # Try with id-id merge\n",
        "    df = movies_df.merge(credits_df, on='id', how='inner')\n",
        "\n",
        "print(f\"âœ… Datasets merged successfully!\")\n",
        "print(f\" Merged Dataset Shape: {df.shape}\")\n",
        "print(f\"ğŸ¬ Total Movies After Merge: {len(df)}\")\n",
        "print(f\"\\n Columns in Merged Dataset:\")\n",
        "print(df.columns.tolist())"
      ],
      "metadata": {
        "id": "oTrUiJvLpQ-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 7: Data Preprocessing - Extract Features"
      ],
      "metadata": {
        "id": "v397lfD1pUbZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def safe_parse_json(x):\n",
        "    \"\"\"Safely parse JSON strings from dataset\"\"\"\n",
        "    try:\n",
        "        if pd.isna(x):\n",
        "            return []\n",
        "        return json.loads(x.replace(\"'\", '\"'))\n",
        "    except:\n",
        "        try:\n",
        "            return ast.literal_eval(x)\n",
        "        except:\n",
        "            return []\n",
        "\n",
        "def extract_names(obj_list, key='name', limit=5):\n",
        "    \"\"\"Extract names from list of dictionaries\"\"\"\n",
        "    try:\n",
        "        names = [item[key] for item in obj_list[:limit] if key in item]\n",
        "        return ', '.join(names)\n",
        "    except:\n",
        "        return ''\n",
        "\n",
        "def extract_director(crew_list):\n",
        "    \"\"\"Extract director name from crew list\"\"\"\n",
        "    try:\n",
        "        for person in crew_list:\n",
        "            if person.get('job') == 'Director':\n",
        "                return person.get('name', '')\n",
        "        return ''\n",
        "    except:\n",
        "        return ''\n",
        "\n",
        "print(\" Starting feature extraction...\")\n",
        "\n",
        "# Fix: Create a unified 'title' column from 'title_x' after merge\n",
        "df['title'] = df['title_x']\n",
        "\n",
        "# Extract genres\n",
        "df['genres_parsed'] = df['genres'].apply(safe_parse_json)\n",
        "df['genres_str'] = df['genres_parsed'].apply(lambda x: extract_names(x))\n",
        "\n",
        "# Extract keywords\n",
        "df['keywords_parsed'] = df['keywords'].apply(safe_parse_json)\n",
        "df['keywords_str'] = df['keywords_parsed'].apply(lambda x: extract_names(x))\n",
        "\n",
        "# Extract cast\n",
        "df['cast_parsed'] = df['cast'].apply(safe_parse_json)\n",
        "df['cast_str'] = df['cast_parsed'].apply(lambda x: extract_names(x, limit=5))\n",
        "\n",
        "# Extract director from crew\n",
        "df['crew_parsed'] = df['crew'].apply(safe_parse_json)\n",
        "df['director'] = df['crew_parsed'].apply(extract_director)\n",
        "\n",
        "print(\" Feature extraction completed!\")\n",
        "print(f\"\\n Sample extracted features:\")\n",
        "print(df[['title', 'genres_str', 'keywords_str', 'cast_str', 'director']].head(3))"
      ],
      "metadata": {
        "id": "-d6dVOIUpXCV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Cell 8: Clean and Handle Missing Values"
      ],
      "metadata": {
        "id": "xLDwHH89pac-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handle missing values\n",
        "print(\" Cleaning data and handling missing values...\")\n",
        "\n",
        "# Fill missing overviews with empty string\n",
        "df['overview'] = df['overview'].fillna('')\n",
        "\n",
        "# Fill other text fields\n",
        "df['genres_str'] = df['genres_str'].fillna('')\n",
        "df['keywords_str'] = df['keywords_str'].fillna('')\n",
        "df['cast_str'] = df['cast_str'].fillna('')\n",
        "df['director'] = df['director'].fillna('')\n",
        "df['title'] = df['title'].fillna('Unknown Title')\n",
        "\n",
        "# Remove duplicates based on title\n",
        "initial_count = len(df)\n",
        "df = df.drop_duplicates(subset=['title'], keep='first')\n",
        "final_count = len(df)\n",
        "\n",
        "print(f\" Data cleaning completed!\")\n",
        "print(f\" Removed {initial_count - final_count} duplicate movies\")\n",
        "print(f\"ğŸ¬ Final dataset size: {final_count} movies\")\n",
        "\n",
        "# Check for any remaining null values\n",
        "print(f\"\\n Null values check:\")\n",
        "print(df[['title', 'overview', 'genres_str', 'keywords_str', 'cast_str', 'director']].isnull().sum())"
      ],
      "metadata": {
        "id": "Q_Spc6P9pc8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 9: Create Combined Search Text"
      ],
      "metadata": {
        "id": "PrdTTnpMqjxA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Create a combined text representation for each movie.\n",
        "This text will be used to generate embeddings for semantic search.\n",
        "\n",
        "Format: title + overview + genres + keywords + cast + director\n",
        "\"\"\"\n",
        "\n",
        "def create_search_text(row):\n",
        "    \"\"\"Combine all relevant features into a single searchable text\"\"\"\n",
        "    components = [\n",
        "        f\"Title: {row['title']}\",\n",
        "        f\"Overview: {row['overview']}\",\n",
        "        f\"Genres: {row['genres_str']}\",\n",
        "        f\"Keywords: {row['keywords_str']}\",\n",
        "        f\"Cast: {row['cast_str']}\",\n",
        "        f\"Director: {row['director']}\"\n",
        "    ]\n",
        "    return ' '.join(components)\n",
        "\n",
        "print(\" Creating combined search text for embeddings...\")\n",
        "\n",
        "df['search_text'] = df.apply(create_search_text, axis=1)\n",
        "\n",
        "print(\"âœ… Search text created successfully!\")\n",
        "print(f\"\\n Sample search text (first 500 characters):\")\n",
        "print(df['search_text'].iloc[0][:500])\n",
        "print(\"\\n...\")\n",
        "print(f\"\\n Average text length: {df['search_text'].str.len().mean():.0f} characters\")"
      ],
      "metadata": {
        "id": "E191LsVIqmwR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 10: Initialize Sentence Transformer Model"
      ],
      "metadata": {
        "id": "Ld5AwjQ9qrRf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Initialize SentenceTransformer for generating embeddings.\n",
        "We use 'all-MiniLM-L6-v2' - a lightweight but powerful model.\n",
        "\n",
        "Model specs:\n",
        "- 384 dimensions\n",
        "- Fast inference\n",
        "- Good for semantic similarity tasks\n",
        "\"\"\"\n",
        "\n",
        "print(\" Loading SentenceTransformer model...\")\n",
        "print(\"Model: all-MiniLM-L6-v2 (384 dimensions)\")\n",
        "\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "print(\"âœ… Model loaded successfully!\")\n",
        "print(f\" Embedding dimension: {model.get_sentence_embedding_dimension()}\")\n",
        "print(f\" Max sequence length: {model.max_seq_length}\")"
      ],
      "metadata": {
        "id": "_7IKZZ8Pquc0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 11: Generate Embeddings"
      ],
      "metadata": {
        "id": "ID4PKnPdqxgj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Generate embeddings for all movies.\n",
        "This is the most computationally intensive step.\n",
        "\n",
        "Process:\n",
        "1. Convert search text to embeddings using SentenceTransformer\n",
        "2. Each movie gets a 384-dimensional vector\n",
        "3. These vectors capture semantic meaning\n",
        "\"\"\"\n",
        "\n",
        "print(\" Generating embeddings for all movies...\")\n",
        "print(\" This may take 2-5 minutes depending on dataset size...\")\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Generate embeddings in batches for efficiency\n",
        "embeddings = model.encode(\n",
        "    df['search_text'].tolist(),\n",
        "    show_progress_bar=True,\n",
        "    batch_size=32,\n",
        "    convert_to_numpy=True\n",
        ")\n",
        "\n",
        "end_time = time.time()\n",
        "elapsed_time = end_time - start_time\n",
        "\n",
        "print(f\"\\nâœ… Embeddings generated successfully!\")\n",
        "print(f\"â± Time taken: {elapsed_time:.2f} seconds\")\n",
        "print(f\" Embeddings shape: {embeddings.shape}\")\n",
        "print(f\" Movies encoded: {len(embeddings)}\")\n",
        "print(f\" Embedding dimensions: {embeddings.shape[1]}\")\n",
        "\n",
        "# Add embeddings to dataframe\n",
        "df['embedding'] = embeddings.tolist()"
      ],
      "metadata": {
        "id": "-rwlW-Mzq0Q3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 12: Initialize ChromaDB with HNSW Index"
      ],
      "metadata": {
        "id": "8Th1-djnq25W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from chromadb.config import Settings\n",
        "import chromadb\n",
        "\n",
        "# --- Initialize persistent Chroma client ---\n",
        "chroma_client = chromadb.PersistentClient(\n",
        "    path=\"./chroma_db\",           # <- folder where chroma.sqlite3 will be stored\n",
        "    settings=Settings(\n",
        "        anonymized_telemetry=False,\n",
        "        allow_reset=True\n",
        "    )\n",
        ")\n",
        "\n",
        "collection_name = \"movie_embeddings\"\n",
        "\n",
        "# Delete if exists (optional)\n",
        "try:\n",
        "    chroma_client.delete_collection(name=collection_name)\n",
        "    print(f\"ğŸ—‘ï¸ Deleted existing collection: {collection_name}\")\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# Create new collection\n",
        "collection = chroma_client.create_collection(\n",
        "    name=collection_name,\n",
        "    metadata={\"hnsw:space\": \"cosine\"}  # Use cosine similarity\n",
        ")\n"
      ],
      "metadata": {
        "id": "UzgpbbLdq5uy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Cell 13: Add Data to ChromaDB"
      ],
      "metadata": {
        "id": "dyBokA_-q8GT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Add movie embeddings to ChromaDB collection.\n",
        "\n",
        "Each document contains:\n",
        "- id: unique movie identifier\n",
        "- embedding: 384-dim vector\n",
        "- metadata: title, overview, cast, director, etc.\n",
        "\"\"\"\n",
        "\n",
        "print(\" Adding movie embeddings to ChromaDB...\")\n",
        "print(\" This may take 1-3 minutes...\")\n",
        "\n",
        "# Prepare data for ChromaDB\n",
        "ids = [str(i) for i in range(len(df))]\n",
        "embeddings_list = embeddings.tolist()\n",
        "\n",
        "# Prepare metadata\n",
        "metadatas = []\n",
        "for idx, row in df.iterrows():\n",
        "    metadatas.append({\n",
        "        'title': row['title'],\n",
        "        'overview': row['overview'][:500],  # Limit overview length\n",
        "        'genres': row['genres_str'],\n",
        "        'keywords': row['keywords_str'],\n",
        "        'cast': row['cast_str'],\n",
        "        'director': row['director']\n",
        "    })\n",
        "\n",
        "# Add to collection in batches\n",
        "batch_size = 500\n",
        "total_batches = (len(ids) + batch_size - 1) // batch_size\n",
        "\n",
        "for i in range(0, len(ids), batch_size):\n",
        "    batch_end = min(i + batch_size, len(ids))\n",
        "\n",
        "    collection.add(\n",
        "        ids=ids[i:batch_end],\n",
        "        embeddings=embeddings_list[i:batch_end],\n",
        "        metadatas=metadatas[i:batch_end]\n",
        "    )\n",
        "\n",
        "    print(f\"âœ“ Added batch {i//batch_size + 1}/{total_batches}\")\n",
        "\n",
        "print(f\"\\nâœ… All {len(ids)} movies added to ChromaDB!\")\n",
        "print(f\" Collection count: {collection.count()}\")"
      ],
      "metadata": {
        "id": "li10cLm5q_MC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 14: Implement Semantic Search Function"
      ],
      "metadata": {
        "id": "3acrd76xrBMU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Semantic Search using Vector Database (ChromaDB + HNSW)\n",
        "\n",
        "Process:\n",
        "1. Convert user query to embedding\n",
        "2. Use ChromaDB's HNSW index for fast ANN search\n",
        "3. Return top-K similar movies with similarity scores\n",
        "\"\"\"\n",
        "\n",
        "def semantic_search(query, top_k=10):\n",
        "    \"\"\"\n",
        "    Perform semantic search using vector database\n",
        "\n",
        "    Args:\n",
        "        query: User's search text\n",
        "        top_k: Number of results to return\n",
        "\n",
        "    Returns:\n",
        "        List of tuples (movie_info, similarity_score)\n",
        "    \"\"\"\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Generate query embedding\n",
        "    query_embedding = model.encode([query])[0].tolist()\n",
        "\n",
        "    # Query ChromaDB\n",
        "    results = collection.query(\n",
        "        query_embeddings=[query_embedding],\n",
        "        n_results=top_k\n",
        "    )\n",
        "\n",
        "    end_time = time.time()\n",
        "    query_time = (end_time - start_time) * 1000  # Convert to ms\n",
        "\n",
        "    # Format results\n",
        "    recommendations = []\n",
        "    for i in range(len(results['ids'][0])):\n",
        "        movie_info = {\n",
        "            'title': results['metadatas'][0][i]['title'],\n",
        "            'overview': results['metadatas'][0][i]['overview'],\n",
        "            'genres': results['metadatas'][0][i]['genres'],\n",
        "            'cast': results['metadatas'][0][i]['cast'],\n",
        "            'director': results['metadatas'][0][i]['director'],\n",
        "            'similarity': 1 - results['distances'][0][i]  # Convert distance to similarity\n",
        "        }\n",
        "        recommendations.append(movie_info)\n",
        "\n",
        "    return recommendations, query_time\n",
        "\n",
        "# Test semantic search\n",
        "print(\" Testing semantic search...\")\n",
        "test_query = \"space adventure with aliens\"\n",
        "results, query_time = semantic_search(test_query, top_k=5)\n",
        "\n",
        "print(f\"\\nâœ… Semantic search working!\")\n",
        "print(f\" Query: '{test_query}'\")\n",
        "print(f\"â± Query time: {query_time:.2f} ms\")\n",
        "print(f\"\\nğŸ¬ Top 5 Results:\")\n",
        "for i, movie in enumerate(results, 1):\n",
        "    print(f\"{i}. {movie['title']} - Similarity: {movie['similarity']*100:.1f}%\")"
      ],
      "metadata": {
        "id": "KWzch9Y_rEp_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 15: Build Collaborative Filtering Similarity Matrix"
      ],
      "metadata": {
        "id": "5tG8O2PYrG5S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Collaborative Filtering (Item-Based)\n",
        "\n",
        "Note: TMDB dataset doesn't have user ratings, so we use content similarity\n",
        "as a proxy for collaborative filtering.\n",
        "\n",
        "Approach:\n",
        "1. Use TF-IDF on combined text features\n",
        "2. Compute item-item similarity matrix\n",
        "3. Recommend movies similar to a given movie\n",
        "\"\"\"\n",
        "\n",
        "print(\" Building collaborative filtering similarity matrix...\")\n",
        "print(\" Computing TF-IDF and similarity matrix...\")\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Create TF-IDF vectors for movie similarity\n",
        "tfidf = TfidfVectorizer(\n",
        "    max_features=5000,\n",
        "    stop_words='english',\n",
        "    ngram_range=(1, 2)\n",
        ")\n",
        "\n",
        "tfidf_matrix = tfidf.fit_transform(df['search_text'])\n",
        "\n",
        "# Compute cosine similarity matrix\n",
        "# Note: For large datasets, this can be memory intensive\n",
        "# We'll compute it in chunks if needed\n",
        "print(f\" TF-IDF matrix shape: {tfidf_matrix.shape}\")\n",
        "\n",
        "# Compute similarity matrix\n",
        "cf_similarity_matrix = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"\\nâœ… Collaborative filtering matrix built!\")\n",
        "print(f\" Time taken: {end_time - start_time:.2f} seconds\")\n",
        "print(f\" Similarity matrix shape: {cf_similarity_matrix.shape}\")\n",
        "\n",
        "# Create movie title to index mapping\n",
        "title_to_idx = {title: idx for idx, title in enumerate(df['title'])}\n",
        "idx_to_title = {idx: title for title, idx in title_to_idx.items()}"
      ],
      "metadata": {
        "id": "dZ4RkLBJrKVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 16: Implement Collaborative Filtering Function"
      ],
      "metadata": {
        "id": "6kn_wm1CrNFE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Collaborative Filtering Recommendation Function\n",
        "\"\"\"\n",
        "\n",
        "def collaborative_filtering(movie_title=None, query=None, top_k=10):\n",
        "    \"\"\"\n",
        "    Perform collaborative filtering recommendation\n",
        "\n",
        "    Args:\n",
        "        movie_title: Specific movie to find similar movies\n",
        "        query: Text query (will find closest movie first)\n",
        "        top_k: Number of recommendations\n",
        "\n",
        "    Returns:\n",
        "        List of recommended movies with similarity scores\n",
        "    \"\"\"\n",
        "    start_time = time.time()\n",
        "\n",
        "    # If query provided, first find the most similar movie\n",
        "    if query and not movie_title:\n",
        "        # Use semantic search to find closest movie\n",
        "        semantic_results, _ = semantic_search(query, top_k=1)\n",
        "        if semantic_results:\n",
        "            movie_title = semantic_results[0]['title']\n",
        "\n",
        "    # Get movie index\n",
        "    if movie_title not in title_to_idx:\n",
        "        return [], 0\n",
        "\n",
        "    movie_idx = title_to_idx[movie_title]\n",
        "\n",
        "    # Get similarity scores for this movie\n",
        "    similarity_scores = cf_similarity_matrix[movie_idx]\n",
        "\n",
        "    # Get top-K similar movies (excluding the movie itself)\n",
        "    similar_indices = similarity_scores.argsort()[::-1][1:top_k+1]\n",
        "\n",
        "    end_time = time.time()\n",
        "    query_time = (end_time - start_time) * 1000\n",
        "\n",
        "    # Format results\n",
        "    recommendations = []\n",
        "    for idx in similar_indices:\n",
        "        movie_data = df.iloc[idx]\n",
        "        movie_info = {\n",
        "            'title': movie_data['title'],\n",
        "            'overview': movie_data['overview'][:500],\n",
        "            'genres': movie_data['genres_str'],\n",
        "            'cast': movie_data['cast_str'],\n",
        "            'director': movie_data['director'],\n",
        "            'similarity': similarity_scores[idx]\n",
        "        }\n",
        "        recommendations.append(movie_info)\n",
        "\n",
        "    return recommendations, query_time\n",
        "\n",
        "# Test collaborative filtering\n",
        "print(\" Testing collaborative filtering...\")\n",
        "test_movie = df['title'].iloc[0]\n",
        "results, query_time = collaborative_filtering(movie_title=test_movie, top_k=5)\n",
        "\n",
        "print(f\"\\nâœ… Collaborative filtering working!\")\n",
        "print(f\" Base movie: '{test_movie}'\")\n",
        "print(f\" Query time: {query_time:.2f} ms\")\n",
        "print(f\"\\nğŸ¬ Top 5 Similar Movies:\")\n",
        "for i, movie in enumerate(results, 1):\n",
        "    print(f\"{i}. {movie['title']} - Similarity: {movie['similarity']*100:.1f}%\")"
      ],
      "metadata": {
        "id": "MJu-jDeYrQcx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 17: Implement Hybrid Recommendation"
      ],
      "metadata": {
        "id": "Hd2EA5olrUxh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Hybrid Recommendation System\n",
        "\n",
        "Combines semantic search and collaborative filtering:\n",
        "Hybrid Score = Î± Ã— Semantic Similarity + (1 - Î±) Ã— CF Similarity\n",
        "\n",
        "where Î± controls the weight (default: 0.7 for semantic, 0.3 for CF)\n",
        "\"\"\"\n",
        "\n",
        "def hybrid_recommendation(query, top_k=10, alpha=0.7):\n",
        "    \"\"\"\n",
        "    Hybrid recommendation combining semantic and collaborative filtering\n",
        "\n",
        "    Args:\n",
        "        query: User search query\n",
        "        top_k: Number of recommendations\n",
        "        alpha: Weight for semantic similarity (0-1)\n",
        "\n",
        "    Returns:\n",
        "        List of recommended movies with combined scores\n",
        "    \"\"\"\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Get semantic search results (more results for better coverage)\n",
        "    semantic_results, _ = semantic_search(query, top_k=top_k*2)\n",
        "\n",
        "    # Get collaborative filtering results\n",
        "    cf_results, _ = collaborative_filtering(query=query, top_k=top_k*2)\n",
        "\n",
        "    # Combine results\n",
        "    combined_scores = {}\n",
        "\n",
        "    # Add semantic scores\n",
        "    for movie in semantic_results:\n",
        "        title = movie['title']\n",
        "        combined_scores[title] = {\n",
        "            'info': movie,\n",
        "            'semantic_score': movie['similarity'],\n",
        "            'cf_score': 0.0\n",
        "        }\n",
        "\n",
        "    # Add CF scores\n",
        "    for movie in cf_results:\n",
        "        title = movie['title']\n",
        "        if title in combined_scores:\n",
        "            combined_scores[title]['cf_score'] = movie['similarity']\n",
        "        else:\n",
        "            combined_scores[title] = {\n",
        "                'info': movie,\n",
        "                'semantic_score': 0.0,\n",
        "                'cf_score': movie['similarity']\n",
        "            }\n",
        "\n",
        "    # Calculate hybrid scores\n",
        "    for title in combined_scores:\n",
        "        semantic_score = combined_scores[title]['semantic_score']\n",
        "        cf_score = combined_scores[title]['cf_score']\n",
        "        hybrid_score = alpha * semantic_score + (1 - alpha) * cf_score\n",
        "        combined_scores[title]['hybrid_score'] = hybrid_score\n",
        "\n",
        "    # Sort by hybrid score\n",
        "    sorted_movies = sorted(\n",
        "        combined_scores.items(),\n",
        "        key=lambda x: x[1]['hybrid_score'],\n",
        "        reverse=True\n",
        "    )[:top_k]\n",
        "\n",
        "    end_time = time.time()\n",
        "    query_time = (end_time - start_time) * 1000\n",
        "\n",
        "    # Format results\n",
        "    recommendations = []\n",
        "    for title, scores in sorted_movies:\n",
        "        movie_info = scores['info'].copy()\n",
        "        movie_info['similarity'] = scores['hybrid_score']\n",
        "        movie_info['semantic_score'] = scores['semantic_score']\n",
        "        movie_info['cf_score'] = scores['cf_score']\n",
        "        recommendations.append(movie_info)\n",
        "\n",
        "    return recommendations, query_time\n",
        "\n",
        "# Test hybrid recommendation\n",
        "print(\"ğŸ§ª Testing hybrid recommendation...\")\n",
        "test_query = \"romantic comedy\"\n",
        "results, query_time = hybrid_recommendation(test_query, top_k=5, alpha=0.7)\n",
        "\n",
        "print(f\"\\nâœ… Hybrid recommendation working!\")\n",
        "print(f\" Query: '{test_query}'\")\n",
        "print(f\" Query time: {query_time:.2f} ms\")\n",
        "print(f\" Alpha (semantic weight): 0.7\")\n",
        "print(f\"\\nğŸ¬ Top 5 Hybrid Results:\")\n",
        "for i, movie in enumerate(results, 1):\n",
        "    print(f\"{i}. {movie['title']}\")\n",
        "    print(f\"   Hybrid: {movie['similarity']*100:.1f}% | Semantic: {movie['semantic_score']*100:.1f}% | CF: {movie['cf_score']*100:.1f}%\")"
      ],
      "metadata": {
        "id": "gmEKtF9CrV8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Cell 18: Save Processed Data"
      ],
      "metadata": {
        "id": "7Q8HaDzxrX-Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Save processed data and models for Streamlit app\n",
        "\"\"\"\n",
        "\n",
        "print(\" Saving processed data...\")\n",
        "\n",
        "# Save movie dataframe\n",
        "df_save = df[['title', 'overview', 'genres_str', 'keywords_str', 'cast_str', 'director']].copy()\n",
        "df_save.to_csv('movies_processed.csv', index=False)\n",
        "\n",
        "# Save title to index mapping\n",
        "import pickle\n",
        "with open('title_to_idx.pkl', 'wb') as f:\n",
        "    pickle.dump(title_to_idx, f)\n",
        "\n",
        "with open('idx_to_title.pkl', 'wb') as f:\n",
        "    pickle.dump(idx_to_title, f)\n",
        "\n",
        "# Save similarity matrix\n",
        "np.save('cf_similarity_matrix.npy', cf_similarity_matrix)\n",
        "\n",
        "print(\"âœ… Data saved successfully!\")\n",
        "print(\" Files created:\")\n",
        "print(\"  - movies_processed.csv\")\n",
        "print(\"  - title_to_idx.pkl\")\n",
        "print(\"  - idx_to_title.pkl\")\n",
        "print(\"  - cf_similarity_matrix.npy\")"
      ],
      "metadata": {
        "id": "5brqUPv9rbco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 19: Create Streamlit App"
      ],
      "metadata": {
        "id": "gnYT4iSxrdiD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# movie_details.py\n",
        "import streamlit as st\n",
        "import requests\n",
        "\n",
        "TMDB_BEARER = \"eyJhbGciOiJIUzI1NiJ9.eyJhdWQiOiIxNTI0NjY1ZGY4YzI5NWU3YzFlZDg1YjQwMDQ2MTg1YyIsIm5iZiI6MTc0NjA0NTgzMC4xMDYsInN1YiI6IjY4MTI4Yjg2MTE1YjkyYTczMmEwZWJhZCIsInNjb3BlcyI6WyJhcGlfcmVhZCJdLCJ2ZXJzaW9uIjoxfQ.iuBlFIRD2TRTPWN1BF7MiJopk3IaAe51zo6mX8q52oM\"\n",
        "TMDB_BASE = \"https://api.themoviedb.org/3\"\n",
        "TMDB_IMAGE_BASE = \"https://image.tmdb.org/t/p/w500\"\n",
        "HEADERS = {\"Authorization\": f\"Bearer {TMDB_BEARER}\"}\n",
        "\n",
        "def get_movie_details(title):\n",
        "    \"\"\"Search movie by title and fetch full details including credits\"\"\"\n",
        "    search_url = f\"{TMDB_BASE}/search/movie\"\n",
        "    r = requests.get(search_url, headers=HEADERS, params={\"query\": title})\n",
        "    data = r.json()\n",
        "    if not data.get(\"results\"):\n",
        "        return None\n",
        "    movie_id = data[\"results\"][0][\"id\"]\n",
        "    details = requests.get(\n",
        "        f\"{TMDB_BASE}/movie/{movie_id}\",\n",
        "        headers=HEADERS,\n",
        "        params={\"append_to_response\": \"credits\"}\n",
        "    ).json()\n",
        "    return details\n",
        "\n",
        "def render_movie_details(movie, recommend_fn, display_fn):\n",
        "    st.markdown(\"<div style='padding-top:0 !important;'></div>\", unsafe_allow_html=True)  # remove top padding\n",
        "    details = get_movie_details(movie[\"title\"])\n",
        "    if not details:\n",
        "        st.error(\"Failed to load movie details\")\n",
        "        return\n",
        "\n",
        "    poster = f\"{TMDB_IMAGE_BASE}{details['poster_path']}\" if details.get(\"poster_path\") else \"\"\n",
        "    genres = \", \".join([g[\"name\"] for g in details.get(\"genres\", [])])\n",
        "    cast = \", \".join([c[\"name\"] for c in details.get(\"credits\", {}).get(\"cast\", [])[:5]])\n",
        "    director = next(\n",
        "        (c[\"name\"] for c in details.get(\"credits\", {}).get(\"crew\", []) if c[\"job\"] == \"Director\"),\n",
        "        \"N/A\"\n",
        "    )\n",
        "\n",
        "    # â”€â”€â”€â”€â”€ Movie Main Card â”€â”€â”€â”€â”€\n",
        "    st.markdown(f\"\"\"\n",
        "    <div style=\"\n",
        "        display:flex;\n",
        "        gap:2rem;\n",
        "        background:rgba(30,41,59,0.85);\n",
        "        padding:1.6rem;\n",
        "        border-radius:18px;\n",
        "    \">\n",
        "        <img src=\"{poster}\" style=\"width:260px;border-radius:14px;\">\n",
        "        <div style=\"color:#e5e7eb;\">\n",
        "            <h2>{details['title']}</h2>\n",
        "            <p><strong>Genres:</strong> {genres}</p>\n",
        "            <p><strong>Rating:</strong> â­ {details['vote_average']}</p>\n",
        "            <p><strong>Runtime:</strong> {details.get('runtime','N/A')} mins</p>\n",
        "            <p><strong>Director:</strong> {director}</p>\n",
        "            <p><strong>Cast:</strong> {cast}</p>\n",
        "            <p style=\"color:#cbd5e1;line-height:1.6;\">\n",
        "                {details.get('overview','No overview')}\n",
        "            </p>\n",
        "        </div>\n",
        "    </div>\n",
        "    \"\"\", unsafe_allow_html=True)\n",
        "\n",
        "    # â”€â”€â”€â”€â”€ Recommendations â”€â”€â”€â”€â”€\n",
        "    st.markdown(\"### ğŸ” Recommended Similar Movies\")\n",
        "    recs, _ = recommend_fn(details.get(\"overview\",\"\"), top_k=6)\n",
        "    recs = [r for r in recs if r['title'].lower() != movie[\"title\"].lower()]\n",
        "\n",
        "    for i, r in enumerate(recs, 1):\n",
        "        display_fn(r, f\"detail_rec_{i}\")\n"
      ],
      "metadata": {
        "id": "Nq4OMOVELxYs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import time\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import chromadb\n",
        "import requests\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Page Config\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "st.set_page_config(\n",
        "    page_title=\" Movie Recommender \",\n",
        "    page_icon=\"ğŸ¬\",\n",
        "    layout=\"wide\",\n",
        "    initial_sidebar_state=\"collapsed\"\n",
        ")\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Session page attribute for CSS\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "page_attr = \"details\" if \"page\" in st.session_state and st.session_state.page == \"details\" else \"home\"\n",
        "st.markdown(f\"\"\"\n",
        "    <script>\n",
        "        document.body.setAttribute('data-page', '{page_attr}');\n",
        "    </script>\n",
        "\"\"\", unsafe_allow_html=True)\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Modern Soft-Glam Dark Theme\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "st.markdown(\"\"\"\n",
        "    <style>\n",
        "    .stApp {\n",
        "        background: linear-gradient(rgba(15,23,42,0.85), rgba(30,41,59,0.85)),\n",
        "                    url(\"https://raw.githubusercontent.com/shaymarin78/Movie-Recommender/main/hero_banner4.jpg\");\n",
        "        background-size: cover;\n",
        "        background-position: center;\n",
        "        background-attachment: fixed;\n",
        "    }\n",
        "\n",
        "    .block-container { max-width: 1100px; padding-top: 1.5rem !important; }\n",
        "    body[data-page=\"details\"] .block-container { padding-top: 0 !important; }\n",
        "\n",
        "    h1, h2, h3 { color: #c7d2fe !important; font-family: 'Segoe UI', sans-serif; }\n",
        "\n",
        "    .movie-card {\n",
        "        background: rgba(30, 41, 59, 0.75);\n",
        "        backdrop-filter: blur(8px);\n",
        "        border-radius: 16px;\n",
        "        padding: 1.2rem;\n",
        "        margin: 0.9rem 0;\n",
        "        border: 1px solid rgba(99, 102, 241, 0.18);\n",
        "        transition: all 0.28s ease;\n",
        "        box-shadow: 0 6px 20px rgba(0,0,0,0.35);\n",
        "        cursor: pointer;\n",
        "    }\n",
        "    .movie-card:hover {\n",
        "        transform: translateY(-6px);\n",
        "        border-color: #818cf8;\n",
        "        box-shadow: 0 12px 32px rgba(99, 102, 241, 0.25);\n",
        "    }\n",
        "    .score-highlight {\n",
        "        font-size: 1.35rem;\n",
        "        font-weight: 700;\n",
        "        color: #a5b4fc;\n",
        "        background: rgba(99, 102, 241, 0.12);\n",
        "        padding: 0.35rem 0.8rem;\n",
        "        border-radius: 12px;\n",
        "        display: inline-block;\n",
        "    }\n",
        "    .stButton > button {\n",
        "        background: linear-gradient(90deg, #6366f1, #818cf8) !important;\n",
        "        color: white !important;\n",
        "        border: none !important;\n",
        "        border-radius: 12px !important;\n",
        "        padding: 0.6rem 1.4rem !important;\n",
        "        font-weight: 600 !important;\n",
        "        transition: all 0.2s;\n",
        "    }\n",
        "    .stButton > button:hover { transform: scale(1.04); box-shadow: 0 4px 15px rgba(99,102,241,0.4) !important; }\n",
        "    .metric-box { background: rgba(30,41,59,0.6); border-radius: 14px; padding: 1.1rem; text-align: center; border: 1px solid #334155; }\n",
        "    hr { border-color: #334155 !important; }\n",
        "    div[data-testid=\"stRadio\"] [role=\"radio\"][aria-checked=\"true\"] > div > div { background-color: #6366f1 !important; border-color: #818cf8 !important; }\n",
        "    div[data-testid=\"stSlider\"] .stSliderTrack { background: linear-gradient(to right, #4f46e5, #818cf8) !important; }\n",
        "    div[data-testid=\"stSlider\"] .stSliderThumb { background-color: #6366f1 !important; border-color: #a5b4fc !important; }\n",
        "    div[data-testid=\"stSlider\"] .stSliderValue { color: #c7d2fe !important; }\n",
        "    .recommendations-grid { display: grid; grid-template-columns: repeat(auto-fill, minmax(240px, 1fr)); gap: 1rem; }\n",
        "    </style>\n",
        "\"\"\", unsafe_allow_html=True)\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# TMDB API Config\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "TMDB_BEARER = \"eyJhbGciOiJIUzI1NiJ9.eyJhdWQiOiIxNTI0NjY1ZGY4YzI5NWU3YzFlZDg1YjQwMDQ2MTg1YyIsIm5iZiI6MTc0NjA0NTgzMC4xMDYsInN1YiI6IjY4MTI4Yjg2MTE1YjkyYTczMmEwZWJhZCIsInNjb3BlcyI6WyJhcGlfcmVhZCJdLCJ2ZXJzaW9uIjoxfQ.iuBlFIRD2TRTPWN1BF7MiJopk3IaAe51zo6mX8q52oM\"\n",
        "TMDB_BASE = \"https://api.themoviedb.org/3\"\n",
        "TMDB_IMAGE_BASE = \"https://image.tmdb.org/t/p/w500\"\n",
        "\n",
        "def get_tmdb_poster(title):\n",
        "    url = f\"{TMDB_BASE}/search/movie\"\n",
        "    headers = {\"Authorization\": f\"Bearer {TMDB_BEARER}\"}\n",
        "    params = {\"query\": title, \"language\": \"en-US\", \"include_adult\": False, \"page\": 1}\n",
        "    try:\n",
        "        res = requests.get(url, headers=headers, params=params)\n",
        "        data = res.json()\n",
        "        if data[\"results\"]:\n",
        "            poster_path = data[\"results\"][0].get(\"poster_path\")\n",
        "            if poster_path:\n",
        "                return f\"{TMDB_IMAGE_BASE}{poster_path}\"\n",
        "    except:\n",
        "        pass\n",
        "    return None\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Session state defaults\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "for key in ['results', 'query_time', 'search_performed', 'method_used']:\n",
        "    if key not in st.session_state:\n",
        "        st.session_state[key] = None if key != 'search_performed' else False\n",
        "if \"page\" not in st.session_state: st.session_state.page = \"home\"\n",
        "if \"selected_movie\" not in st.session_state: st.session_state.selected_movie = None\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Load resources\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "@st.cache_resource(show_spinner=\"Loading intelligence...\")\n",
        "def load_resources():\n",
        "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "    df = pd.read_csv('movies_processed.csv')\n",
        "    df['overview'] = df['overview'].astype(str).fillna('')\n",
        "    with open('title_to_idx.pkl', 'rb') as f:\n",
        "        title_to_idx = pickle.load(f)\n",
        "    with open('idx_to_title.pkl', 'rb') as f:\n",
        "        _ = pickle.load(f)\n",
        "    cf_sim = np.load('cf_similarity_matrix.npy')\n",
        "    client = chromadb.PersistentClient(path=\"/content/chroma_db\")\n",
        "    collection = client.get_collection(\"movie_embeddings\")\n",
        "    return model, df, title_to_idx, cf_sim, collection\n",
        "\n",
        "model, df, title_to_idx, cf_similarity_matrix, collection = load_resources()\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Recommendation Functions\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Core search functions (same as before but cleaner)\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "def semantic_search(query, top_k=10):\n",
        "    start = time.time()\n",
        "    q_emb = model.encode([query])[0].tolist()\n",
        "    res = collection.query(query_embeddings=[q_emb], n_results=top_k)\n",
        "    recs = []\n",
        "    for i, meta in enumerate(res['metadatas'][0]):\n",
        "        recs.append({\n",
        "            'title': meta.get('title', 'Unknown'),\n",
        "            'overview': meta.get('overview', 'No overview available'),\n",
        "            'genres': meta.get('genres', 'N/A'),\n",
        "            'cast': meta.get('cast', 'N/A'),\n",
        "            'director': meta.get('director', 'N/A'),\n",
        "            'similarity': 1 - res['distances'][0][i]\n",
        "        })\n",
        "    return recs, (time.time() - start) * 1000\n",
        "\n",
        "def collaborative_filtering(query, top_k=10):\n",
        "    sem_res, _ = semantic_search(query, 1)\n",
        "    if not sem_res: return [], 0\n",
        "    title = sem_res[0]['title']\n",
        "    if title not in title_to_idx: return [], 0\n",
        "\n",
        "    start = time.time()\n",
        "    idx = title_to_idx[title]\n",
        "    scores = cf_similarity_matrix[idx]\n",
        "    sim_idx = scores.argsort()[::-1][1:top_k+1]\n",
        "\n",
        "    recs = []\n",
        "    for i in sim_idx:\n",
        "        row = df.iloc[i]\n",
        "        recs.append({\n",
        "            'title': row['title'],\n",
        "            'overview': row['overview'],\n",
        "            'genres': row.get('genres', 'N/A'),\n",
        "            'cast': row.get('cast', 'N/A'),\n",
        "            'director': row.get('director', 'N/A'),\n",
        "            'similarity': scores[i]\n",
        "        })\n",
        "    return recs, (time.time() - start) * 1000\n",
        "\n",
        "def hybrid_recommendation(query, top_k=10, alpha=0.7):\n",
        "    sem, _ = semantic_search(query, top_k*4)\n",
        "    cf, _  = collaborative_filtering(query, top_k*4)\n",
        "    start = time.time()\n",
        "\n",
        "    scores = {}\n",
        "    for m in sem: scores[m['title']] = {'sem': m['similarity'], 'cf': 0.0, 'info': m}\n",
        "    for m in cf:\n",
        "        t = m['title']\n",
        "        if t in scores: scores[t]['cf'] = m['similarity']\n",
        "        else: scores[t] = {'sem': 0.0, 'cf': m['similarity'], 'info': m}\n",
        "\n",
        "    for t in scores:\n",
        "        scores[t]['hybrid'] = alpha * scores[t]['sem'] + (1 - alpha) * scores[t]['cf']\n",
        "\n",
        "    sorted_items = sorted(scores.items(), key=lambda x: x[1]['hybrid'], reverse=True)[:top_k]\n",
        "    recs = [data['info'].copy() | {'similarity': data['hybrid']} for _, data in sorted_items]\n",
        "    return recs, (time.time() - start) * 1000\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Display Movie Card\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "def display_movie_card(movie, rank, context=\"home\"):\n",
        "    poster_url = get_tmdb_poster(movie['title'])\n",
        "    with st.container():\n",
        "        st.markdown(f\"\"\"\n",
        "        <div class=\"movie-card\">\n",
        "            <h3 style=\"margin:0 0 0.6rem 0;\">{rank}. {movie['title']}</h3>\n",
        "            <div class=\"score-highlight\">{movie['similarity']:.3f}</div>\n",
        "            <div style=\"display:flex; gap:1rem; margin-top:0.6rem;\">\n",
        "                {'<img src=\"'+poster_url+'\" width=\"120\">' if poster_url else ''}\n",
        "                <div>\n",
        "                    <p style=\"margin:0.8rem 0 0.4rem 0; color:#cbd5e1;\">\n",
        "                        <strong>Genres:</strong> {movie.get('genres', 'N/A')}<br>\n",
        "                        <strong>Cast:</strong> {movie.get('cast', 'N/A')}<br>\n",
        "                        <strong>Director:</strong> {movie.get('director', 'N/A')}\n",
        "                    </p>\n",
        "                    <p style=\"color:#94a3b8; line-height:1.45;\">\n",
        "                        {movie.get('overview', 'No overview available')[:240]}{'...' if len(movie.get('overview','')) > 240 else ''}\n",
        "                    </p>\n",
        "                </div>\n",
        "            </div>\n",
        "        </div>\n",
        "        \"\"\", unsafe_allow_html=True)\n",
        "\n",
        "    btn_key = f\"{context}_{movie['title']}\"\n",
        "    if st.button(\"View details\", key=btn_key):\n",
        "        st.session_state.selected_movie = movie\n",
        "        st.session_state.page = \"details\"\n",
        "        st.stop()\n",
        "\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# HOME PAGE LOGIC\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "if st.session_state.page == \"home\":\n",
        "\n",
        "    st.title(\"ğŸ¬ Movie Recommender\")\n",
        "    st.markdown(\"**Discover your next favorite movie** â€” Semantic Â· Collaborative Â· Hybrid\")\n",
        "\n",
        "    with st.container():\n",
        "        col_method, col_k, col_alpha, col_search = st.columns([2.2, 1.1, 1.3, 1.1])\n",
        "\n",
        "        with col_method:\n",
        "            method = st.radio(\"Method\", [\"Semantic\", \"Collaborative\", \"Hybrid\"], horizontal=True, label_visibility=\"collapsed\")\n",
        "\n",
        "        with col_k:\n",
        "            top_k_options = [4, 6, 8, 10, 12, 14, 16, 20, 30]\n",
        "            top_k = st.selectbox(\"Results count\", top_k_options, index=2, label_visibility=\"collapsed\")\n",
        "\n",
        "        with col_alpha:\n",
        "            alpha = 0.7\n",
        "            if method == \"Hybrid\":\n",
        "                alpha = st.slider(\"Î± (Semantic weight)\", 0.0, 1.0, 0.7, 0.05, label_visibility=\"collapsed\")\n",
        "\n",
        "        with col_search:\n",
        "            st.write(\"\")\n",
        "            search_trigger = st.button(\"Find Movies\", type=\"primary\", use_container_width=True)\n",
        "\n",
        "    query = st.text_input(\"What are you in the mood for?\",\n",
        "                          placeholder=\"space adventure with aliens â€¢ dark thriller like Inception â€¢ feel-good romance\",\n",
        "                          key=\"main_query\", label_visibility=\"collapsed\")\n",
        "\n",
        "    examples = [\n",
        "        \"space adventure with aliens\",\n",
        "        \"dark  thriller about dreams\",\n",
        "        \"romantic comedy Leonardo DiCaprio\",\n",
        "        \"Marvel style superhero action\",\n",
        "        \"historical war drama\"\n",
        "    ]\n",
        "    st.markdown(\"**Quick tries:**\")\n",
        "    ex_cols = st.columns(5)\n",
        "    for i, txt in enumerate(examples):\n",
        "        if ex_cols[i].button(txt, key=f\"ex_{i}\", use_container_width=True):\n",
        "            query = txt\n",
        "            search_trigger = True\n",
        "\n",
        "    if search_trigger and query.strip():\n",
        "      with st.spinner(\"Computing recommendations for all methods...\"):\n",
        "        # Compute results for all methods\n",
        "        sem_results, sem_time = semantic_search(query, top_k)\n",
        "        cf_results, cf_time = collaborative_filtering(query, top_k)\n",
        "        hyb_results, hyb_time = hybrid_recommendation(query, top_k, alpha)\n",
        "\n",
        "        # Store all results\n",
        "        st.session_state.results_all = {\n",
        "            \"Semantic\": {\"results\": sem_results, \"time\": sem_time},\n",
        "            \"Collaborative\": {\"results\": cf_results, \"time\": cf_time},\n",
        "            \"Hybrid\": {\"results\": hyb_results, \"time\": hyb_time},\n",
        "        }\n",
        "\n",
        "        # Store currently selected method for display\n",
        "        st.session_state.results = (\n",
        "            sem_results if method==\"Semantic\" else\n",
        "            cf_results if method==\"Collaborative\" else\n",
        "            hyb_results\n",
        "        )\n",
        "        st.session_state.query_time = (\n",
        "            sem_time if method==\"Semantic\" else\n",
        "            cf_time if method==\"Collaborative\" else\n",
        "            hyb_time\n",
        "        )\n",
        "        st.session_state.method_used = method\n",
        "        st.session_state.search_performed = True\n",
        "        st.session_state.last_query = query\n",
        "\n",
        "\n",
        "    if st.session_state.search_performed and st.session_state.results:\n",
        "        st.markdown(\"---\")\n",
        "        st.subheader(f\"Results for: **{st.session_state.last_query}**  Â·  {st.session_state.method_used}\")\n",
        "        m1, m2, m3 = st.columns(3)\n",
        "        m1.metric(\"Speed\", f\"{st.session_state.query_time:.1f} ms\")\n",
        "        m2.metric(\"Found\", len(st.session_state.results))\n",
        "        avg = np.mean([m['similarity'] for m in st.session_state.results])\n",
        "        m3.metric(\"Avg Match\", f\"{avg:.3f}\")\n",
        "\n",
        "        for i, mov in enumerate(st.session_state.results, 1):\n",
        "            display_movie_card(mov, i, context=\"home\")\n",
        "    elif st.session_state.search_performed:\n",
        "        st.info(\"No strong matches. Try rephrasing or switch method.\")\n",
        "\n",
        "    st.markdown(\"<br><small style='color:#64748b;'>Semantic Search (ChromaDB + Sentence Transformers) Â· Collaborative Filtering Â· Hybrid Approach<br>Built with Streamlit |2025</small>\", unsafe_allow_html=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# Evaluation & Comparison Tab (Dynamic)\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "st.markdown(\"---\")\n",
        "\n",
        "tab_results, tab_eval = st.tabs([\"Results\", \"Evaluation & Comparison\"])\n",
        "\n",
        "with tab_eval:\n",
        "    st.subheader(\"Method Comparison & Metrics\")\n",
        "\n",
        "    if st.session_state.search_performed and hasattr(st.session_state, \"results_all\"):\n",
        "        data = {\n",
        "            \"Method\": [], \"Avg Similarity\": [], \"Query Time (ms)\": [],\n",
        "            \"Precision@10\": [], \"Recall@10\": [], \"Diversity\": [], \"Cold-start handling\": []\n",
        "        }\n",
        "\n",
        "        for m_name, m_data in st.session_state.results_all.items():\n",
        "            res = m_data['results']\n",
        "            qtime = m_data['time']\n",
        "\n",
        "            if res:\n",
        "                sim_scores = np.array([r['similarity'] for r in res])\n",
        "                avg_sim = float(sim_scores.mean())\n",
        "\n",
        "                K = min(10, len(res))\n",
        "                top_k_scores = np.sort(sim_scores)[-K:]\n",
        "\n",
        "                # Method-wise precision@K: fraction of top-K above 0.5\n",
        "                precision_at_k = float((top_k_scores >= 0.25).sum() / K)\n",
        "\n",
        "                # Method-wise recall@K: fraction of all results above 0.5\n",
        "                recall_at_k = float((sim_scores >= 0.25).sum() / len(sim_scores))\n",
        "\n",
        "                # Diversity\n",
        "                all_genres = [genre for r in res for genre in r['genres']]\n",
        "                unique_genres = len(set(all_genres))\n",
        "                if unique_genres > 4:\n",
        "                    diversity = \"High\"\n",
        "                elif unique_genres >= 2:\n",
        "                    diversity = \"Medium\"\n",
        "                else:\n",
        "                    diversity = \"Low\"\n",
        "\n",
        "                # Cold-start dynamically\n",
        "                if precision_at_k >= 0.7:\n",
        "                    cold_start = \"Good\"\n",
        "                elif precision_at_k >= 0.5:\n",
        "                    cold_start = \"Medium\"\n",
        "                else:\n",
        "                    cold_start = \"Poor\"\n",
        "            else:\n",
        "                avg_sim = precision_at_k = recall_at_k = 0\n",
        "                diversity = \"Low\"\n",
        "                cold_start = \"Poor\"\n",
        "\n",
        "            data[\"Method\"].append(m_name)\n",
        "            data[\"Avg Similarity\"].append(avg_sim)\n",
        "            data[\"Query Time (ms)\"].append(qtime)\n",
        "            data[\"Precision@10\"].append(precision_at_k)\n",
        "            data[\"Recall@10\"].append(recall_at_k)\n",
        "            data[\"Diversity\"].append(diversity)\n",
        "            data[\"Cold-start handling\"].append(cold_start)\n",
        "\n",
        "        # Display\n",
        "        df_metrics = pd.DataFrame(data)\n",
        "        highlight_cols = [\"Avg Similarity\", \"Precision@10\", \"Recall@10\"]\n",
        "        st.dataframe(df_metrics.style.highlight_max(subset=highlight_cols, color=\"#6366f140\"),\n",
        "                     use_container_width=True)\n",
        "\n",
        "        # Bar chart\n",
        "        fig, ax = plt.subplots(figsize=(8,4))\n",
        "        x = np.arange(len(df_metrics))\n",
        "        width = 0.25\n",
        "        ax.bar(x - width, df_metrics[\"Precision@10\"], width, label=\"Precision@10\", color=\"#6366f1\")\n",
        "        ax.bar(x, df_metrics[\"Recall@10\"], width, label=\"Recall@10\", color=\"#a5b4fc\")\n",
        "        ax.bar(x + width, df_metrics[\"Avg Similarity\"], width, label=\"Avg Similarity\", color=\"#c7d2fe\")\n",
        "        ax.set_xticks(x)\n",
        "        ax.set_xticklabels(df_metrics[\"Method\"])\n",
        "        ax.set_ylim(0, 1)\n",
        "        ax.set_ylabel(\"Score\")\n",
        "        ax.set_title(\"Evaluation Metrics for Last Search\")\n",
        "        ax.legend()\n",
        "        ax.grid(axis='y', alpha=0.3)\n",
        "        st.pyplot(fig)\n",
        "\n",
        "        st.caption(\"\"\"\n",
        "Precision & Recall are calculated **method-wise using each method's own similarity scores**:\n",
        "- Semantic â†’ similarity from semantic search\n",
        "- Collaborative â†’ similarity from CF results\n",
        "- Hybrid â†’ weighted similarity (hybrid score)\n",
        "This avoids all-zero or all-one issues and reflects actual performance per method.\n",
        "\"\"\")\n",
        "    else:\n",
        "        st.info(\"Metrics will appear here after performing a search.\")\n",
        "\n",
        "#st.markdown(\"<br><small style='color:#64748b;'>Semantic Search (ChromaDB + Sentence Transformers) Â· Collaborative Filtering Â· Hybrid Approach<br>Built with Streamlit |2025</small>\", unsafe_allow_html=True)\n",
        "\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "# DETAILS PAGE LOGIC\n",
        "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "if st.session_state.page == \"details\":\n",
        "    st.markdown(\"<div style='margin-top:20px;'>\", unsafe_allow_html=True)  # Add top margin\n",
        "    if st.button(\"â† Back\"):\n",
        "        st.session_state.page = \"home\"\n",
        "        st.session_state.selected_movie = None\n",
        "        st.stop()\n",
        "    st.markdown(\"</div>\", unsafe_allow_html=True)\n",
        "\n",
        "\n",
        "    movie = st.session_state.selected_movie\n",
        "\n",
        "    st.subheader(movie[\"title\"])\n",
        "    poster = get_tmdb_poster(movie[\"title\"])\n",
        "    if poster:\n",
        "        st.image(poster, width=260)\n",
        "\n",
        "    st.markdown(f\"\"\"\n",
        "    **Genres:** {movie.get(\"genres\",\"N/A\")}\n",
        "    **Cast:** {movie.get(\"cast\",\"N/A\")}\n",
        "    **Director:** {movie.get(\"director\",\"N/A\")}\n",
        "    **Overview:**\n",
        "    {movie.get(\"overview\",\"\")}\n",
        "    \"\"\")\n",
        "\n",
        "    st.markdown(\"---\")\n",
        "    st.markdown(\"### ğŸ” Recommended based on this movie\")\n",
        "\n",
        "    recs, _ = hybrid_recommendation(movie.get(\"overview\",\"\"), top_k=6)\n",
        "    recs = [r for r in recs if r['title'] != movie['title']]  # remove current movie\n",
        "\n",
        "    # Wrap recommendations in a grid container\n",
        "    st.markdown('<div class=\"recommendations-grid\">', unsafe_allow_html=True)\n",
        "    for i, r in enumerate(recs, 1):\n",
        "        display_movie_card(r, i, context=\"details\")\n",
        "    st.markdown('</div>', unsafe_allow_html=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "Lre1wU2I4sqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pkill -f streamlit\n",
        "!pkill -f cloudflared"
      ],
      "metadata": {
        "id": "VE9GhAb6yHSj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cell 20: Launch Streamlit App"
      ],
      "metadata": {
        "id": "oPB9Px8eQPO-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 20: Launch Streamlit App with Public URL (Cloudflared - NO PASSWORD)\n",
        "\n",
        "print(\"ğŸš€ Launching Streamlit App with Cloudflared (No Password)...\")\n",
        "\n",
        "import os, time, subprocess, re\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "# Check app.py\n",
        "if not os.path.exists(\"app.py\"):\n",
        "    raise FileNotFoundError(\"âŒ app.py not found. Please run the previous cell.\")\n",
        "\n",
        "print(\"âœ… app.py found\")\n",
        "\n",
        "# Install cloudflared\n",
        "print(\" Installing cloudflared...\")\n",
        "!wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64 -O cloudflared\n",
        "!chmod +x cloudflared\n",
        "!mv cloudflared /usr/local/bin/\n",
        "\n",
        "# Start Streamlit\n",
        "print(\"ğŸŒŸ Starting Streamlit server...\")\n",
        "get_ipython().system_raw(\"streamlit run app.py --server.port 8501 &\")\n",
        "\n",
        "time.sleep(10)\n",
        "\n",
        "# Start Cloudflared tunnel\n",
        "print(\" Creating public URL...\")\n",
        "get_ipython().system_raw(\"cloudflared tunnel --url http://localhost:8501 --no-autoupdate > cloudflared.log 2>&1 &\")\n",
        "\n",
        "time.sleep(8)\n",
        "\n",
        "# Extract URL\n",
        "with open(\"cloudflared.log\") as f:\n",
        "    log = f.read()\n",
        "\n",
        "match = re.search(r\"https://[-\\w]+\\.trycloudflare\\.com\", log)\n",
        "\n",
        "if match:\n",
        "    public_url = match.group(0)\n",
        "    print(\"=\"*80)\n",
        "    print(\" STREAMLIT APP IS LIVE (NO PASSWORD REQUIRED)\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\" PUBLIC URL: {public_url}\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    display(HTML(\n",
        "        f'<h2>ğŸ¬ <a href=\"{public_url}\" target=\"_blank\">'\n",
        "        'Click Here to Open Movie Recommendation App</a></h2>'\n",
        "    ))\n",
        "else:\n",
        "    print(\"âŒ Failed to generate URL. Check cloudflared.log\")\n"
      ],
      "metadata": {
        "id": "LE4v2dIxSMSA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "86gxAf9oBm0V"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}